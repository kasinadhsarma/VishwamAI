{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAgaG5e3zM6I"
      },
      "source": [
        "# VishwamAI Pretraining on Google Colab\n",
        "\n",
        "This notebook provides an optimized linear pipeline for pretraining VishwamAI's 671B parameter model.\n",
        "\n",
        "**Model Architecture:**\n",
        "- Parameters: 671B\n",
        "- Context Length: 32,768 tokens\n",
        "- Hidden Size: 8,192\n",
        "- Attention Heads: 64\n",
        "- Layers: 120\n",
        "- Vocabulary Size: 64,000\n",
        "\n",
        "**Pipeline Steps & Timing:**\n",
        "1. Setup (~2 min)\n",
        "2. Authentication (~30 sec)\n",
        "3. Model Loading (~2 min)\n",
        "4. Dataset Loading (~10 min)\n",
        "5. Training (~1 hour/epoch)\n",
        "6. Model Pushing (~10 min)\n",
        "\n",
        "Total Expected Time: ~4 hours for 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1tACofDazM6L"
      },
      "outputs": [],
      "source": [
        "# Progress tracking setup\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import Trainer\n",
        "\n",
        "def track_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f\"Operation completed in {end - start:.2f} seconds\")\n",
        "        return result\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC3k12ZNzM6M"
      },
      "source": [
        "# 1. Fast Setup (≈2 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RM0OTCDzM6N",
        "outputId": "bde604ea-1ef2-4f4b-825c-402b03a56f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb 14 06:43:12 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "⚠️ Warning: This model requires an A100 GPU for optimal performance\n",
            "Current GPU: Tesla T4\n",
            "CPU times: user 20.7 ms, sys: 30.5 ms, total: 51.1 ms\n",
            "Wall time: 191 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Verify GPU availability and requirements\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "if 'A100' not in gpu_name:\n",
        "    print(\"⚠️ Warning: This model requires an A100 GPU for optimal performance\")\n",
        "    print(\"Current GPU:\", gpu_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcVzrox6zM6O",
        "outputId": "230c2e12-756e-440f-c370-ea2f831c4482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.6/857.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.34.0 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.34.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 214 ms, sys: 28.6 ms, total: 242 ms\n",
            "Wall time: 19.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Parallel package installation\n",
        "%pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
        "    transformers==4.34.0 datasets accelerate huggingface_hub wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lYwaDaizM6O"
      },
      "source": [
        "# 2. Quick Authentication (≈30 sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "0PFdDSsVzM6O",
        "outputId": "6a4cf921-cac6-453a-ba00-3bfa464431ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face access token: ··········\n",
            "Successfully logged in to Hugging Face!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maivishwam\u001b[0m (\u001b[33maivishwam-vishwamai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Weights & Biases!\n",
            "CPU times: user 3.05 s, sys: 473 ms, total: 3.53 s\n",
            "Wall time: 1min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from huggingface_hub import login, create_repo\n",
        "from getpass import getpass\n",
        "import wandb\n",
        "\n",
        "# Get token securely\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "login(token=hf_token)\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Initialize W&B for experiment tracking\n",
        "wandb.login()\n",
        "print(\"Successfully logged in to Weights & Biases!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWqLFuLLzM6P",
        "outputId": "2ba5d87c-1647-455a-effe-c47f0b377e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VishwamAI'...\n",
            "remote: Enumerating objects: 949, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (274/274), done.\u001b[K\n",
            "remote: Total 949 (delta 137), reused 272 (delta 76), pack-reused 595 (from 2)\u001b[K\n",
            "Receiving objects: 100% (949/949), 28.41 MiB | 26.28 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n",
            "/content/VishwamAI\n",
            "\u001b[31mERROR: file:///content/VishwamAI does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mCPU times: user 35.1 ms, sys: 4.6 ms, total: 39.7 ms\n",
            "Wall time: 3.32 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Quick repository setup\n",
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI\n",
        "%pip install -e . -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToSKgPAg0H6m",
        "outputId": "8f822414-caea-4e9f-bd91-81ffc548c0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuheI6bxzM6P"
      },
      "source": [
        "# 3. Model Configuration (≈2 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeVJZmBszM6Q",
        "outputId": "daa72459-1a66-4a54-9ce0-12daf319dc95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 32.2 ms, sys: 1.91 ms, total: 34.1 ms\n",
            "Wall time: 79.1 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from vishwamai.model_utils import load_model, get_gpu_memory\n",
        "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
        "from vishwamai.cache_augmentation import DifferentiableCacheAugmentation, CacheConfig\n",
        "from huggingface_hub import HfFolder, Repository\n",
        "\n",
        "# Performance optimizations\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iltT1hwfzM6Q",
        "outputId": "c17ce851-b6d4-4433-f349-ec1f529f4711"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/kasinadhsarma/vishwamai-model into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/kasinadhsarma/vishwamai-model into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "repo_name = \"kasinadhsarma/vishwamai-model\"  # Existing repo\n",
        "repo = Repository(local_dir=\"./vishwamai-model\", clone_from=repo_name, use_auth_token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxWdUcQJzM6Q",
        "outputId": "7803ed6c-f3f6-476b-c17b-201ae8924f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4 (14.7 GB)\n",
            "Operation completed in 0.00 seconds\n"
          ]
        }
      ],
      "source": [
        "@track_time\n",
        "def setup_hardware():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = get_gpu_memory()\n",
        "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "\n",
        "    # Optimize for available GPU\n",
        "    if 'a100' in gpu_name.lower():\n",
        "        return 'A100_optimized', 128, 65536  # Full 671B model\n",
        "    elif 'v100' in gpu_name.lower():\n",
        "        return 'V100_optimized', 64, 32768   # Reduced size\n",
        "    else:\n",
        "        return 'T4_optimized', 32, 16384     # Minimal configuration\n",
        "\n",
        "gpu_type, expert_count, cache_size = setup_hardware()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0yF3hBJzM6R",
        "outputId": "2e66d4ed-d1e9-4bfe-82f6-5f38d1342d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config keys: dict_keys(['model_type', 'architectures', 'vocab_size', 'hidden_size', 'intermediate_size', 'num_attention_heads', 'num_hidden_layers', 'max_position_embeddings', 'tree_depth', 'branch_factor', 'use_conceptual_tokens', 'concept_embedding_size', 'use_fp8', 'attention_dropout', 'hidden_dropout', 'max_concepts_per_token', 'initializer_range', 'layer_norm_epsilon', 'use_cache', 'pad_token_id', 'bos_token_id', 'eos_token_id', 'tie_word_embeddings', 'rope_scaling', 'attention_config', 'training_config', 'colab_specific'])\n",
            "Operation completed in 0.00 seconds\n",
            "Configuration loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "@track_time\n",
        "def load_config():\n",
        "    config_path = \"./vishwamai/configs/config_671b.json\"\n",
        "\n",
        "    # Load JSON file\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Debugging: Print keys to verify structure\n",
        "    print(\"Config keys:\", config.keys())\n",
        "\n",
        "    # Ensure 'model_config' exists\n",
        "    if \"model_config\" not in config:\n",
        "        config[\"model_config\"] = {}\n",
        "\n",
        "    # Ensure 'gpu_type' exists in 'colab_specific'\n",
        "    if \"colab_specific\" not in config or gpu_type not in config[\"colab_specific\"]:\n",
        "        raise KeyError(f\"GPU type '{gpu_type}' not found in config['colab_specific']. Available: {list(config.get('colab_specific', {}).keys())}\")\n",
        "\n",
        "    gpu_config = config[\"colab_specific\"][gpu_type]\n",
        "\n",
        "    # Update model configuration dynamically\n",
        "    config[\"model_config\"].update({\n",
        "        \"dim\": 8192,\n",
        "        \"num_attention_heads\": 64,\n",
        "        \"num_hidden_layers\": 120,\n",
        "        \"vocab_size\": 64000,\n",
        "        \"max_position_embeddings\": 32768,\n",
        "        \"batch_size\": gpu_config.get(\"batch_size\", 8),\n",
        "        \"num_experts\": expert_count,\n",
        "        \"experts_per_token\": min(16, expert_count // 8),\n",
        "        \"memory_size\": gpu_config.get(\"memory_size\", 2048),\n",
        "        \"tree_beam_width\": gpu_config.get(\"tree_beam_width\", 4),\n",
        "        \"cache_size\": cache_size\n",
        "    })\n",
        "\n",
        "    return config, gpu_config\n",
        "\n",
        "# Load configuration\n",
        "config, gpu_config = load_config()\n",
        "print(\"Configuration loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTS85s_uzM6R"
      },
      "source": [
        "# 4. Dataset Loading (≈10 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "9addcc3b0748457ca101757c0924fc26",
            "cf3243c398294f06a23e191d33033ac7",
            "97a4ae5f4e1a469a9508da6e0385d39d",
            "9830ae0e8ece4f0bb535dcfe3173eb28",
            "5d33297241724ac8a302b527777c8a3d",
            "a62d1ef89fec429aadba90354cde62a4",
            "52cc2a9afa4a4b9fa71308e2567ba4a4",
            "490568c7fd9b49eaa7fa3388a796e69c",
            "cf66a813ddbf46388c2e474cdc0d9a53",
            "c56bbe446d8143139cc96e3304bee01a",
            "8723e657bcb443e2af2149cbb5ce03a9"
          ]
        },
        "id": "bKX_0l-SzM6R",
        "outputId": "dbdd6491-6857-4caa-b4c1-02885346ecc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9addcc3b0748457ca101757c0924fc26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Failed to load gsm8k: ParquetConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load mmlu: ParquetConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load mmlu_pro: BuilderConfig ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={'test': ['data/test-*'], 'validation': ['data/validation-*']}, description=None, batch_size=None, columns=None, features=None, filters=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load mmmlu: BuilderConfig CsvConfig(name='default', version=0.0.0, data_dir=None, data_files={'test': ['test/*.csv']}, description=None, sep=',', delimiter=None, header='infer', names=None, column_names=None, index_col=None, usecols=None, prefix=None, mangle_dupe_cols=True, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, thousands=None, decimal='.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, memory_map=False, float_precision=None, chunksize=10000, features=None, encoding_errors='strict', on_bad_lines='error', date_format=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load mmmu: ParquetConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load leetcode1: BuilderConfig JsonConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/greengerong/leetcode@00f2d466dc0f00f65a0b6938c4c11a57f721db81/leetcode-train.jsonl']}, description=None, features=None, encoding='utf-8', encoding_errors=None, field=None, use_threads=True, block_size=None, chunksize=10485760, newlines_in_values=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load leetcode2: Dataset 'LimYeri/LeetCode_Python_Solutions_v2' is a gated dataset on the Hub. Visit the dataset page at https://huggingface.co/datasets/LimYeri/LeetCode_Python_Solutions_v2 to ask for access.\n",
            "Warning: Failed to load leetcode3: BuilderConfig JsonConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/newfacade/LeetCodeDataset@cd1cff71c7750188678b21dde6676d3c31275f93/LeetCodeDataset-v1-train-problems.jsonl'], NamedSplit('test'): ['hf://datasets/newfacade/LeetCodeDataset@cd1cff71c7750188678b21dde6676d3c31275f93/LeetCodeDataset-v1-test-problems.jsonl', 'hf://datasets/newfacade/LeetCodeDataset@cd1cff71c7750188678b21dde6676d3c31275f93/LeetCodeDataset-v2-test-problems.jsonl']}, description=None, features=None, encoding='utf-8', encoding_errors=None, field=None, use_threads=True, block_size=None, chunksize=10485760, newlines_in_values=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load math: BuilderConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load ifeval: BuilderConfig JsonConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/google/IFEval@966cd89545d6b6acfd7638bc708b98261ca58e84/ifeval_input_data.jsonl']}, description=None, features=None, encoding='utf-8', encoding_errors=None, field=None, use_threads=True, block_size=None, chunksize=10485760, newlines_in_values=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load gpqa: CsvConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load frames: BuilderConfig CsvConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('test'): ['hf://datasets/google/frames-benchmark@58d9fb6330f3ab1316d1eca12e5e8ef23dcc22ef/test.tsv']}, description=None, sep='\\t', delimiter=None, header='infer', names=None, column_names=None, index_col=None, usecols=None, prefix=None, mangle_dupe_cols=True, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, thousands=None, decimal='.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, memory_map=False, float_precision=None, chunksize=10000, features=None, encoding_errors='strict', on_bad_lines='error', date_format=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load camel_math: BuilderConfig JsonConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/camel-ai/math@c1b9a23cfbb77b34668df5705b6e8832a06046c1/math.zip']}, description=None, features=None, encoding='utf-8', encoding_errors=None, field=None, use_threads=True, block_size=None, chunksize=10485760, newlines_in_values=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load camel_code: BuilderConfig JsonConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/camel-ai/code@1f291f9d51cc4c5f4a127ea9ed945ffa64ada329/code_chat.zip', 'hf://datasets/camel-ai/code@1f291f9d51cc4c5f4a127ea9ed945ffa64ada329/code_instructions.json']}, description=None, features=None, encoding='utf-8', encoding_errors=None, field=None, use_threads=True, block_size=None, chunksize=10485760, newlines_in_values=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load scbench: ParquetConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "Warning: Failed to load swe_bench: BuilderConfig ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={'test': ['data/test-*']}, description=None, batch_size=None, columns=None, features=None, filters=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load swe_bench_full: BuilderConfig ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={'dev': ['data/dev-*'], 'test': ['data/test-*'], 'train': ['data/train-*']}, description=None, batch_size=None, columns=None, features=None, filters=None) doesn't have a 'use_auth_token' key.\n",
            "Warning: Failed to load wikipedia: ParquetConfig.__init__() got an unexpected keyword argument 'use_auth_token'\n",
            "\n",
            "Dataset sizes:\n",
            "Operation completed in 32.47 seconds\n"
          ]
        }
      ],
      "source": [
        "@track_time\n",
        "def load_parallel_datasets():\n",
        "    datasets = {}\n",
        "    print(\"Loading datasets...\")\n",
        "    dataset_configs = [\n",
        "        (\"gsm8k\", \"openai/gsm8k\", \"train\"),\n",
        "        (\"mmlu\", \"cais/mmlu\", \"train\"),\n",
        "        (\"mmlu_pro\", \"TIGER-Lab/MMLU-Pro\", \"train\"),\n",
        "        (\"mmmlu\", \"openai/MMMLU\", \"train\"),\n",
        "        (\"mmmu\", \"MMMU/MMMU\", \"train\"),\n",
        "        (\"leetcode1\", \"greengerong/leetcode\", \"train\"),\n",
        "        (\"leetcode2\", \"LimYeri/LeetCode_Python_Solutions_v2\", \"train\"),\n",
        "        (\"leetcode3\", \"newfacade/LeetCodeDataset\", \"train\"),\n",
        "        (\"math\", \"deepmind/math_dataset\", \"train\"),\n",
        "        (\"ifeval\", \"google/IFEval\", \"train\"),\n",
        "        (\"gpqa\", \"Idavidrein/gpqa\", \"train\"),\n",
        "        (\"frames\", \"google/frames-benchmark\", \"train\"),\n",
        "        (\"camel_math\", \"camel-ai/math\", \"train\"),\n",
        "        (\"camel_code\", \"camel-ai/code\", \"train\"),\n",
        "        (\"scbench\", \"microsoft/SCBench\", \"train\"),\n",
        "        (\"swe_bench\", \"princeton-nlp/SWE-bench_Verified\", \"train\"),\n",
        "        (\"swe_bench_full\", \"princeton-nlp/SWE-bench\", \"train\"),\n",
        "        (\"wikipedia\", \"wikimedia/wikipedia\", \"train\")\n",
        "    ]\n",
        "\n",
        "    with tqdm(total=len(dataset_configs)) as pbar:\n",
        "        for name, dataset_id, split in dataset_configs:\n",
        "            try:\n",
        "                datasets[name] = load_dataset(dataset_id, split=split, use_auth_token=True)\n",
        "                pbar.update(1)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to load {name}: {str(e)}\")\n",
        "\n",
        "    print(\"\\nDataset sizes:\")\n",
        "    for name, dataset in datasets.items():\n",
        "        print(f\"{name}: {len(dataset):,} examples\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "datasets = load_parallel_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ysgj91zM6S"
      },
      "source": [
        "# 5. Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "w9dgI4xazM6S",
        "outputId": "948cdfef-3592-42ff-bca6-a72047496c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing 671B parameter model...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "CacheConfig.__init__() got an unexpected keyword argument 'max_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bed49c6c19fd>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_thoughts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_thoughts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nModel size: {sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-56e2ddace7e1>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Operation completed in {end - start:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-bed49c6c19fd>\u001b[0m in \u001b[0;36minitialize_components\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing 671B parameter model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     model = load_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./vishwamai/configs/config_671b.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VishwamAI/vishwamai/model_utils.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(config_path, device, pretrained_path, use_cache)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVishwamaiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained_path\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VishwamAI/vishwamai/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnParallelLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VishwamAI/vishwamai/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnParallelLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VishwamAI/vishwamai/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVishwamaiConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/VishwamAI/vishwamai/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache_augmentation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             self.cache = DifferentiableCacheAugmentation(\n\u001b[0;32m--> 217\u001b[0;31m                 CacheConfig(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_num_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CacheConfig.__init__() got an unexpected keyword argument 'max_length'"
          ]
        }
      ],
      "source": [
        "@track_time\n",
        "def initialize_components():\n",
        "    print(\"Initializing 671B parameter model...\")\n",
        "\n",
        "    model = load_model(\n",
        "        config_path=\"./vishwamai/configs/config_671b.json\",\n",
        "        device=\"cuda\",\n",
        "        use_cache=False\n",
        "    )\n",
        "\n",
        "    memory = NeuralMemory(\n",
        "        dim=config['model_config']['dim'],\n",
        "        memory_size=config['model_config']['memory_size']\n",
        "    )\n",
        "\n",
        "    tree_thoughts = TreeOfThoughts(\n",
        "        model=model,\n",
        "        beam_width=config['model_config']['tree_beam_width']\n",
        "    )\n",
        "\n",
        "    cache = DifferentiableCacheAugmentation(\n",
        "        CacheConfig(\n",
        "            hidden_size=config['model_config']['dim'],\n",
        "            num_heads=8,  # Using reasonable default\n",
        "            dropout=0.1,  # Using reasonable default\n",
        "            max_cache_length=config['model_config']['cache_size']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return model, memory, tree_thoughts, cache\n",
        "\n",
        "model, memory, tree_thoughts, cache = initialize_components()\n",
        "\n",
        "print(f\"\\nModel size: {sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\")\n",
        "print(f\"Memory slots: {config['model_config']['memory_size']:,}\")\n",
        "print(f\"Cache entries: {config['model_config']['cache_size']:,}\")\n",
        "print(f\"Context length: {config['model_config']['max_position_embeddings']:,} tokens\")\n",
        "print(f\"Active experts: {config['model_config']['experts_per_token']} per token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP7GS6f5zM6S"
      },
      "source": [
        "# 6. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_fgMU0tzM6S"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Initialize output directory and repository\n",
        "output_dir = \"./pretrain_output\"\n",
        "!mkdir -p $output_dir\n",
        "\n",
        "repo = Repository(\n",
        "    local_dir=output_dir,\n",
        "    clone_from=repo_name,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "# Configure training with FSDP optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=gpu_config['batch_size'],\n",
        "    gradient_accumulation_steps=gpu_config['gradient_accumulation'],\n",
        "    learning_rate=1.2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=1000,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # Distributed training\n",
        "    fsdp=\"full_shard\",\n",
        "    fsdp_transformer_layer_cls_to_wrap=\"VishwamAILayer\",\n",
        "    # Performance optimizations\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True,\n",
        "    group_by_length=True,\n",
        "    # Features\n",
        "    use_moe=True,\n",
        "    use_neural_memory=True,\n",
        "    use_tree_of_thoughts=True,\n",
        "    use_cache_augmentation=True,\n",
        "    # Monitoring\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    # Hub integration\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=repo_name,\n",
        "    hub_strategy=\"every_save\",\n",
        "    # Other optimizations\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=1.0,\n",
        "    length_penalty=1.0,\n",
        "    early_stopping=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMm18U81zM6T"
      },
      "source": [
        "# 7. Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-1Vya2TzM6T"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Combine selected datasets for training\n",
        "train_datasets = []\n",
        "for ds_name in [\"gsm8k\", \"leetcode1\", \"leetcode2\", \"math\"]:\n",
        "    if ds_name in datasets:\n",
        "        train_datasets.append(datasets[ds_name])\n",
        "if not train_datasets:\n",
        "    raise ValueError(\"No available training datasets found for pretraining.\")\n",
        "\n",
        "combined_train_dataset = concatenate_datasets(train_datasets)\n",
        "\n",
        "# Select a development (evaluation) dataset, e.g., mmlu. Fall back to mmlu_pro if needed.\n",
        "development_dataset = datasets.get(\"mmlu\") or datasets.get(\"mmlu_pro\")\n",
        "\n",
        "trainer = VishwamAIPretrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=combined_train_dataset,\n",
        "    eval_dataset=development_dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc2Sr66szM6U"
      },
      "outputs": [],
      "source": [
        "# Start training with monitoring\n",
        "print(\"Starting pretraining pipeline...\")\n",
        "start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nPretraining completed in {training_time/3600:.2f} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SCgZgsLzM6U"
      },
      "source": [
        "# 8. Model Saving and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwjLcCHuzM6U"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def save_model_components():\n",
        "    model_save_path = \"final_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    memory.save_pretrained(f\"{model_save_path}/memory\")\n",
        "    tree_thoughts.save_pretrained(f\"{model_save_path}/tree_thoughts\")\n",
        "    cache.save_pretrained(f\"{model_save_path}/cache\")\n",
        "\n",
        "    # Push to Hugging Face Hub\n",
        "    trainer.push_to_hub()\n",
        "    return model_save_path\n",
        "\n",
        "model_save_path = save_model_components()\n",
        "print(f\"Model available at: https://huggingface.co/{repo_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPJu8t4lzM6U"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def validate_model():\n",
        "    test_model = load_model(\n",
        "        config_path=\"configs/config_671b.json\",\n",
        "        device=\"cuda\",\n",
        "        pretrained_path=model_save_path\n",
        "    )\n",
        "\n",
        "    test_cases = [\n",
        "        \"Solve this math problem: What is the area of a circle with radius 5?\",\n",
        "        \"Explain the concept of quantum entanglement.\",\n",
        "        \"Write a Python function to find the nth Fibonacci number using dynamic programming.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Running validation tests...\")\n",
        "    for test_input in test_cases:\n",
        "        print(f\"\\nTest: {test_input}\")\n",
        "        encoded = model.tokenizer.encode(test_input, return_tensors=\"pt\").cuda()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            start = time.time()\n",
        "            output = test_model.generate(\n",
        "                encoded,\n",
        "                max_new_tokens=200,\n",
        "                num_beams=4,\n",
        "                temperature=0.7,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            end = time.time()\n",
        "\n",
        "        response = model.tokenizer.decode(output[0])\n",
        "        print(f\"Response (generated in {end-start:.2f}s):\")\n",
        "        print(response)\n",
        "\n",
        "validate_model()\n",
        "print(\"\\nPretraining and validation completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "490568c7fd9b49eaa7fa3388a796e69c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52cc2a9afa4a4b9fa71308e2567ba4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d33297241724ac8a302b527777c8a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8723e657bcb443e2af2149cbb5ce03a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97a4ae5f4e1a469a9508da6e0385d39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_490568c7fd9b49eaa7fa3388a796e69c",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf66a813ddbf46388c2e474cdc0d9a53",
            "value": 0
          }
        },
        "9830ae0e8ece4f0bb535dcfe3173eb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c56bbe446d8143139cc96e3304bee01a",
            "placeholder": "​",
            "style": "IPY_MODEL_8723e657bcb443e2af2149cbb5ce03a9",
            "value": " 0/18 [00:32&lt;?, ?it/s]"
          }
        },
        "9addcc3b0748457ca101757c0924fc26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf3243c398294f06a23e191d33033ac7",
              "IPY_MODEL_97a4ae5f4e1a469a9508da6e0385d39d",
              "IPY_MODEL_9830ae0e8ece4f0bb535dcfe3173eb28"
            ],
            "layout": "IPY_MODEL_5d33297241724ac8a302b527777c8a3d"
          }
        },
        "a62d1ef89fec429aadba90354cde62a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56bbe446d8143139cc96e3304bee01a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf3243c398294f06a23e191d33033ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a62d1ef89fec429aadba90354cde62a4",
            "placeholder": "​",
            "style": "IPY_MODEL_52cc2a9afa4a4b9fa71308e2567ba4a4",
            "value": "  0%"
          }
        },
        "cf66a813ddbf46388c2e474cdc0d9a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}