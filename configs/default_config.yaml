# Model configuration
embed_dim: 2048
num_heads: 32
num_layers: 24
vocab_size: 50000
dropout_rate: 0.1
ff_dim: 8192
pad_token_id: 0

# Training configuration
learning_rate: 1e-4
weight_decay: 0.01
batch_size: 32
num_epochs: 10
max_seq_length: 2048

# Data configuration
train_file: "data/processed/train.txt"
eval_file: "data/processed/eval.txt"
tokenizer_name: "gpt2"